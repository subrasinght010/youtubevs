{
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "#@title Installing Packages\n",
        "from IPython.display import clear_output\n",
        "!pip install mysqlclient\n",
        "clear_output()"
      ],
      "metadata": {
        "cellView": "form",
        "id": "b20CFe7-xJnz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Importing Packages and UDFs\n",
        "\n",
        "import csv\n",
        "import os\n",
        "import googleapiclient.discovery\n",
        "from googleapiclient.errors import HttpError\n",
        "import time\n",
        "import pandas as pd\n",
        "import datetime\n",
        "import MySQLdb\n",
        "from google.colab import files\n",
        "\n",
        "def get_cnx():\n",
        "    return MySQLdb.connect(host='**********', user='*********', password='*******@', db='*******')\n",
        "\n",
        "def insert_records(cnx, query, records):\n",
        "\n",
        "    try:\n",
        "        cursor = cnx.cursor()\n",
        "        cursor.executemany(query, records)\n",
        "        cnx.commit()\n",
        "    except MySQLdb.OperationalError:\n",
        "        cnx = get_cnx()\n",
        "        cursor = cnx.cursor()\n",
        "        cursor.executemany(query, records)\n",
        "        cnx.commit()\n",
        "\n",
        "class YoutubeScraper:\n",
        "\n",
        "    def build_youtube_client(self,api_keys):\n",
        "        \"\"\"Builds the YouTube client object using the API key.\"\"\"\n",
        "        return googleapiclient.discovery.build(\"youtube\", \"v3\", developerKey=api_keys)\n",
        "\n",
        "\n",
        "    def generate_published_dates(self):\n",
        "          \n",
        "        now = datetime.datetime.now()\n",
        "        year_range = range(2010, now.year+1)\n",
        "        published_before = []\n",
        "        published_after = []\n",
        "\n",
        "        for year in year_range:\n",
        "            if year != 2023:\n",
        "                published_after.append(datetime.datetime(year, 1, 1).strftime('%Y-%m-%d'))\n",
        "                published_before.append(datetime.datetime(year, 4, 30).strftime('%Y-%m-%d'))\n",
        "                published_after.append(datetime.datetime(year, 4, 1).strftime('%Y-%m-%d'))\n",
        "                published_before.append(datetime.datetime(year, 8, 30).strftime('%Y-%m-%d'))\n",
        "                published_after.append(datetime.datetime(year, 8, 1).strftime('%Y-%m-%d'))\n",
        "                published_before.append(datetime.datetime(year, 12, 30).strftime('%Y-%m-%d'))\n",
        "\n",
        "\n",
        "        published_after.append(datetime.datetime(year, 1, 1).strftime('%Y-%m-%d'))\n",
        "        published_before.append(datetime.datetime(year, 3, 30).strftime('%Y-%m-%d'))\n",
        "        return published_before[::-1], published_after[::-1]\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "    def retrieve_videos(self, keyword, limit,api_keys_list):\n",
        "        \"\"\"Retrieves YouTube videos with the given keyword using one of the API keys in the list.\"\"\"\n",
        "\n",
        "        published_Before,published_After=self.generate_published_dates()\n",
        "\n",
        "        regions = ['IN','US', 'AU','UK','CA', 'NZ','IE']\n",
        "\n",
        "        cnx = get_cnx()\n",
        "        cursor = cnx.cursor()\n",
        "\n",
        "        cursor.execute(\"SELECT video_id from urls;\")\n",
        "        result = cursor.fetchall()\n",
        "        video_id_list = [res[0] for res in result]\n",
        "\n",
        "        timeindex,rindex=0,0\n",
        "        index=0\n",
        "\n",
        "        youtube=self.build_youtube_client(api_keys_list[index])\n",
        "        videos = []\n",
        "        video_ids = set()\n",
        "        page_num = 0\n",
        "        page_token = ''\n",
        "        while len(videos) < limit:\n",
        "            try:\n",
        "                search_response = youtube.search().list(\n",
        "                    part=\"snippet,id\",\n",
        "                    q=keyword,\n",
        "                    regionCode=f\"{regions[rindex]}\",\n",
        "                    #videoCaption=\"closedCaption\",\n",
        "                    type=\"playlist,video\",\n",
        "                    publishedAfter=f'{published_After[timeindex]}T00:00:00Z',\n",
        "                    publishedBefore=f'{published_Before[timeindex]}T23:59:59Z',\n",
        "                    maxResults=min(50, limit - len(videos)),\n",
        "                    pageToken=page_token,).execute()\n",
        "                \n",
        "\n",
        "                # Retrieve video details for each search result\n",
        "                video_details = []\n",
        "                for search_result in search_response.get(\"items\", []):\n",
        "                    if \"videoId\" in search_result[\"id\"]:\n",
        "                        video_id = search_result[\"id\"][\"videoId\"]\n",
        "                        if video_id not in video_ids and video_id not in video_id_list:\n",
        "                            video_ids.add(video_id)\n",
        "                            video_details.append(video_id)\n",
        "\n",
        "                video_details = youtube.videos().list(\n",
        "                    part=\"id,snippet\",\n",
        "                    id=\",\".join(video_details)\n",
        "                ).execute()\n",
        "                print(video_details)\n",
        "\n",
        "\n",
        "\n",
        "                # Store video data in a list\n",
        "                for video_result in video_details.get(\"items\", []):\n",
        "                    video_title = video_result[\"snippet\"][\"title\"]\n",
        "                    video_description = video_result[\"snippet\"][\"description\"]\n",
        "                    if video_description:\n",
        "                      video_id = video_result['id']\n",
        "                      video_link = f\"https://www.youtube.com/watch?v={video_id}\"\n",
        "                      videos.append([video_link, video_title, video_description, video_id])\n",
        "                  \n",
        "                if len(videos) > limit:\n",
        "                  self.save_to_csv(videos, keyword)\n",
        "                  print(\"stoping...\")\n",
        "                  break\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "                if \"nextPageToken\" not in search_response:\n",
        "                    print(\"Current YouTube object has been exhausted. No more pages to search!\")\n",
        "                    page_token = ''\n",
        "                    timeindex+= 1\n",
        "                    if timeindex>=len(published_Before):\n",
        "                      rindex+=1\n",
        "                      \n",
        "                      if rindex>=len(regions):\n",
        "                        print(\"No More Regions Existing...\")\n",
        "                        self.save_to_csv(videos,keyword)\n",
        "                        break\n",
        "\n",
        "                      timeindex=0\n",
        "\n",
        "                    time.sleep(8)\n",
        "                    youtube = self.build_youtube_client(api_keys_list[index])\n",
        "\n",
        "                \n",
        "\n",
        "  \n",
        "                page_token = search_response.get(\"nextPageToken\")\n",
        "                page_num += 1\n",
        "                print(f\"Scraped page {page_num} \", \"videos--> \", len(videos))\n",
        "                # Add a delay to avoid exceeding the API quota\n",
        "                time.sleep(2)\n",
        "                if len(videos)>=limit:\n",
        "                  print(f\"{limit} videos successfully scraped. Exiting...\") \n",
        "\n",
        "\n",
        "            except HttpError as error:\n",
        "                self.save_to_csv(videos, keyword)\n",
        "                print(f\"Error retrieving video data: {error}\")\n",
        "                if error.resp.status == 403:\n",
        "                  index+=1\n",
        "                  if index>=len(api_keys_list):\n",
        "                      print(\"All API keys have been exhausted. Exiting...\")\n",
        "\n",
        "                      break\n",
        "                  print(f\"API has been changed . Now videos are Retrieving with...{api_keys_list[index]}\")\n",
        "                  youtube=self.build_youtube_client(api_keys_list[index])\n",
        "            \n",
        "                \n",
        "\n",
        "        return videos, video_ids\n",
        "\n",
        "    def remove_duplicates(self, keyword):\n",
        "        file_name = f\"{keyword}_videos.csv\"\n",
        "        df = pd.read_csv(file_name)\n",
        "        df = df.drop_duplicates()\n",
        "        df = df.dropna()\n",
        "        df.to_csv(file_name, index=False)\n",
        "        return file_name, df.shape[0]\n",
        "\n",
        "    def save_to_csv(self,videos, keyword):\n",
        "        file_name = f\"{keyword}_videos.csv\"\n",
        "        # Check if file exists, append to it\n",
        "        mode = \"a\" if os.path.exists(file_name) else \"w\"\n",
        "        with open(file_name, mode, newline=\"\", encoding=\"utf-8\") as csv_file:\n",
        "            writer = csv.writer(csv_file)\n",
        "            if mode == \"w\":\n",
        "                writer.writerow([\"Link\", \"Title\", \"Description\",\"video_id\"])\n",
        "            writer.writerows(videos)\n",
        "\n",
        "# Define function to scrape YouTube videos\n",
        "def completeVidoes_scraper(api_keys, keyword, num_videos):\n",
        "\n",
        "    youtube_scraper = YoutubeScraper()\n",
        "\n",
        "    videos, video_ids = youtube_scraper.retrieve_videos(keyword, num_videos, api_keys)\n",
        "    youtube_scraper.save_to_csv(videos,keyword)\n",
        "    filename, total_records = youtube_scraper.remove_duplicates(keyword)\n",
        "\n",
        "    cnx = get_cnx()\n",
        "    insert_query = 'INSERT INTO urls (video_id) VALUES(%s);'\n",
        "    insert_records(cnx, insert_query, video_ids)\n",
        "    print(f\"Compeleted! Total of {total_records} URLs have Been Saved in {filename}.\\nFile is Downloading...\")\n",
        "    files.download(filename)"
      ],
      "metadata": {
        "id": "HCewJZhAfvJh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "10#@title Run Function\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    # Prompt user to enter keyword, number of videos, and number of API keys\n",
        "    keyword = input(\"Enter Keyword...\")\n",
        "    num_videos = int(input(\"Enter Number of Videos...\"))\n",
        "\n",
        "    # Suggest using up to 4 API keys for scraping up to 10,000 videos\n",
        "    if num_videos > 10000:\n",
        "        print(\"For scraping more than 10,000 videos, it's recommended to use up to 4 API keys.\")\n",
        "\n",
        "    api_keys = []\n",
        "\n",
        "    print(\"Wait, we will update you...\")\n",
        "\n",
        "    completeVidoes_scraper(api_keys, keyword, num_videos)"
      ],
      "metadata": {
        "id": "Yr4Fgm3g4QxQ"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}